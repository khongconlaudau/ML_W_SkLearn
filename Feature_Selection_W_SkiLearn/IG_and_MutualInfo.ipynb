{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f66319-c0b9-46df-9558-c60fbd90fbc0",
   "metadata": {},
   "source": [
    "# Information Gain and Mutual Information for Machine Learning\n",
    "- Information Gain (IG) is a measure used in decision trees to quantify the effectiveness of a feature in splitting the dataset into classes. It calculates the reduction in entropy (uncertainty) of the target variable (class labels) when a particular feature is known.\n",
    "- In simpler terms, Information Gain helps us understand how much a particular feature contributes to making accurate predictions in a decision tree. Features with higher Information Gain are considered more informative and are preferred for splitting the dataset, as they lead to nodes with more homogenous classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6dccf-60d8-48d6-85ae-92a144cb85fc",
   "metadata": {},
   "source": [
    "<img src=\"../assets/IG_Form.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582c5092-0b4d-43c2-b35d-82df71640bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for each feature: [0.50089081 0.22195978 0.98038801 0.9949308 ]\n"
     ]
    }
   ],
   "source": [
    "# Implementation in Python\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Calculate Information Gain using mutual_info_classif\n",
    "info_gain = mutual_info_classif(X ,y)\n",
    "print(\"Information Gain for each feature:\", info_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa9532-1c96-48f9-8735-e289421b7e3a",
   "metadata": {},
   "source": [
    "- The output represents the Information Gain for each feature in the Iris dataset, which contains four features: sepal length, sepal width, petal length, and petal width.\n",
    "- Information Gain values are in the range of 0 to 1, where higher values indicate features that are more informative or relevant for predicting the target variable (flower species in this case).\n",
    "    - First feature (sepal length) is approximately 0.506.\n",
    "    - Second feature (sepal width) is approximately 0.273.\n",
    "    - Third feature (petal length) is approximately 0.995.\n",
    "    - Fourth feature (petal width) is approximately 0.985.\n",
    "- Based on these Information Gain values, we can infer that petal length and petal width are highly informative features compared to sepal length and sepal width for predicting the species of Iris flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633e1c4-abf3-4879-85e6-dc69c39577c4",
   "metadata": {},
   "source": [
    "### Advantages of Information Gain (IG)\n",
    "- Simple to Compute: IG is straightforward to calculate, making it easy to implement in machine learning algorithms.\n",
    "- Effective for Feature Selection: IG is particularly useful in decision tree algorithms for selecting the most informative features, which can improve model accuracy and reduce overfitting.\n",
    "- Interpretability: The concept of IG is intuitive and easy to understand, as it measures how much knowing a feature reduces uncertainty in predicting the target variable.\n",
    "\n",
    "### Limitation \n",
    "- Ignores Feature Interactions: IG treats features independently and may not consider interactions between features, potentially missing important relationships that could improve model performance.\n",
    "- Biased Towards Features with Many Categories: Features with a large number of categories or levels may have higher IG simply due to their granularity, leading to bias in feature selection towards such features. (Like user's id or zipcode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66306d23-c4f1-47b6-aff3-b0c64ecaa5f6",
   "metadata": {},
   "source": [
    "## What is Mutual Information?\n",
    "- Mutual Information (MI) is a measure of the mutual dependence between two random variables. In the context of machine learning, MI quantifies the amount of information obtained about one variable through the other variable. It is a non-negative value that indicates the degree of dependence between the variables: the higher the MI, the greater the dependence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f17acd-b77f-4b78-8377-b81c25cc80cb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/MI_Form.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3132220-3372-427b-a064-ef33163e7a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information for each feature: [0.42283584 0.54090791]\n"
     ]
    }
   ],
   "source": [
    "# Implementation in Python\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np \n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)\n",
    "y = X[:, 0] + np.sin(6 * np.pi * X[:, 1])\n",
    "\n",
    "# Calculate Mutual Information using mutual_info_regression\n",
    "mutual_info = mutual_info_regression(X, y)\n",
    "print(\"Mutual Information for each feature:\", mutual_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c309070-33ed-40d9-b10f-c92cfb6a3de5",
   "metadata": {},
   "source": [
    "- The output represents the Mutual Information for each feature in a dataset with two features.\n",
    "    - Mutual Information for the first feature is approximately 0.423.\n",
    "    - Second feature is approximately 0.541.\n",
    "- Higher Mutual Information values suggest a stronger relationship or dependency between the features and the target variable.\n",
    "- So, the Mutual Information values indicate the amount of information each feature provides about the target variable (y), which is a combination of the first feature and a sine function of the second feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d53890-979a-408e-be51-8d431728cf90",
   "metadata": {},
   "source": [
    "### Advantages of Mutual Information (MI)\n",
    "- Captures Nonlinear Relationships: MI can capture both linear and nonlinear relationships between variables, making it suitable for identifying complex dependencies in the data.\n",
    "- Versatile: MI can be used in various machine learning tasks such as feature selection, clustering, and dimensionality reduction, providing valuable insights into the relationships between variables.\n",
    "- Handles Continuous and Discrete Variables: MI is effective for both continuous and discrete variables, making it applicable to a wide range of datasets.\n",
    "\n",
    "### Limitations of Mutual Information (MI)\n",
    "- Sensitive to Feature Scaling: MI can be sensitive to feature scaling, where the magnitude or range of values in different features may affect the calculated mutual information values.\n",
    "- Affected by Noise: MI may be influenced by noise or irrelevant features in the dataset, potentially leading to overestimation or underestimation of the true dependencies between variables.\n",
    "- Computational Complexity: Calculating MI for large datasets with many features can be computationally intensive, especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d6e34-a4c9-4bdd-8fd1-39ea4a3134d4",
   "metadata": {},
   "source": [
    "<img src=\"../assets/IG_vs_MI.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b4b35-820f-44a9-a769-0fe271d02cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
